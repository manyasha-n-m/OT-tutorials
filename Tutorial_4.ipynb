{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMC/G6g6W7uiGIhrQLHbfgV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manyasha-n-m/OT-tutorials/blob/main/Tutorial_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hands-on Tutorials on Computational Optimal Transport\n",
        "### Instructor: Nataliia Monina\n",
        "-----\n",
        "\n",
        "### 4. Dynamics and Sampling problems\n",
        "In this tutorial:\n",
        "-  We will take a look at a particular example of what was discussed in gradient flows course.\n",
        "- Suppose you have a dataset of samples `x_real` from some distribution (e.g. pictures of cats, dogs, digits or some other type of data in general) and you want to find a way to somehow generate fake samples `x_fake` that very much resemble the true distribution.\n",
        "    1. Trying to teach a neural network the distribution by comraring the empirical measures of real and generated samples via (entropic) Wasserstein distance\n",
        "    2. Using theory of Wasserstein gradient flows: Flowing a source measure $\\rho_0$ to the target measure $\\pi$.\n"
      ],
      "metadata": {
        "id": "u7bKe0zTkDoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "from ipywidgets import interact, FloatSlider, IntSlider\n",
        "from scipy.stats import gaussian_kde\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input, Lambda, Flatten, Concatenate, Dropout\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, MaxPooling2D, Reshape\n",
        "from tensorflow.keras import Model"
      ],
      "metadata": {
        "id": "R4sbqFhj0ucY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(mnist_digits, mnist_labels), (_, _) = mnist.load_data()"
      ],
      "metadata": {
        "id": "Jbr37CxtamgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displacement interpolation, Wasserstein geodesics:\n",
        "Example: Think that you want to track the evolution of the particles in the system: at time $t=0$ you see that they are distributed according to some $\\mu_0$ and then you track the particles at time $t=1$ and now you see distribution $\\mu_1$. You may think: how exactly did my particles move to end up like this?\n",
        "\n",
        "Mathematically, you take a look at the flow of initial measure $\\mu_0$ towards $\\mu_1$.\n",
        "\n",
        "#### First, we will take a simple example in 1D.\n",
        "\n",
        "**Recall:**\n",
        "\n",
        "Use Brenierâ€™s theorem in 1D: In 1D, the optimal transport (Monge) map is explicitly\n",
        "$$T(x) = F^{-1}_1(F_0(X))$$\n",
        "where $F_0$ and $F_1$ are, respectively, the CDFs of source  and target measures $\\mu_0$ and $\\mu_1$.\n",
        "\n",
        "We now already know that the geodesic curve is given by\n",
        "$$\\mu_t = ((1-t)Id + t T)_\\sharp \\mu_0$$"
      ],
      "metadata": {
        "id": "GA0AaZE2fYSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "n = 500\n",
        "x = np.linspace(-5, 10, 500)\n",
        "\n",
        "# Source and target distributions\n",
        "mu0_pdf = norm(loc=0, scale=1)\n",
        "mu1_pdf = norm(loc=5, scale=2)\n",
        "\n",
        "eps = 1e-6\n",
        "u = np.linspace(eps, 1 - eps, n)\n",
        "\n",
        "# Inverse CDF sampling (via quantile function)\n",
        "x0_samples = mu0_pdf.ppf(u)\n",
        "x1_samples = mu1_pdf.ppf(u)\n",
        "\n",
        "# Interpolation function using samples\n",
        "def plot_displacement_interpolation(t=0.5):\n",
        "    xt = (1 - t) * x0_samples + t * x1_samples  # displacement interpolation\n",
        "\n",
        "    # Estimate density using KDE\n",
        "    kde = gaussian_kde(xt)\n",
        "    mu_t = kde(x)\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(x, mu0_pdf.pdf(x), label=r'$\\mu_0$ (Source)', color='blue')\n",
        "    plt.plot(x, mu1_pdf.pdf(x), label=r'$\\mu_1$ (Target)', color='green')\n",
        "    plt.plot(x, mu_t, label=fr'$\\mu_t$,  t = {t:.2f}', color='red')\n",
        "    plt.title(\"Displacement Interpolation (via OT map)\")\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"Density\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# 3. Interactive slider\n",
        "interact(plot_displacement_interpolation, t=FloatSlider(value=0.5, min=0.0, max=1.0, step=0.01));"
      ],
      "metadata": {
        "id": "SPfY15f5rPSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Now let's look at 2D example"
      ],
      "metadata": {
        "id": "-un7im6vlBOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate two 2D point clouds\n",
        "np.random.seed(42)\n",
        "n_points = 50\n",
        "X0 = np.random.randn(n_points, 2) * 0.5 + np.array([0, 0])    # source\n",
        "X1 = np.random.randn(n_points, 2) * 0.5 + np.array([3, 3])    # target\n",
        "\n",
        "# Sort for consistent visualization\n",
        "# will NOT give an OT-optimal map, but a possibility to see!!!!\n",
        "X0_sorted = X0[np.argsort(X0[:, 0])]\n",
        "X1_sorted = X1[np.argsort(X1[:, 0])]\n",
        "\n",
        "def plot_interp(t):\n",
        "    X_t = (1 - t) * X0_sorted + t * X1_sorted # test plan T_t(x)!\n",
        "\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.scatter(X0_sorted[:, 0], X0_sorted[:, 1], color='blue', label=r'$\\mu_0$')\n",
        "    plt.scatter(X1_sorted[:, 0], X1_sorted[:, 1], color='red', label=r'$\\mu_1$')\n",
        "    plt.scatter(X_t[:, 0], X_t[:, 1], color='green', label=fr'$\\mu_t$, $t={t:.2f}$')\n",
        "\n",
        "    for i in range(n_points):\n",
        "        plt.plot([X0_sorted[i, 0], X1_sorted[i, 0]],\n",
        "                 [X0_sorted[i, 1], X1_sorted[i, 1]], 'k--', alpha=0.1)\n",
        "\n",
        "    plt.title(\"Displacement Interpolation between Point Clouds\")\n",
        "    plt.legend()\n",
        "    plt.axis('equal')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Interactive widget\n",
        "interact(plot_interp, t=FloatSlider(value=0.5, min=0.0, max=1.0, step=0.01));"
      ],
      "metadata": {
        "id": "mlBDTJMXkvED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercises:\n",
        "- Try to solve OT between Unif(X_0) and Unif(X_1) (i.e. construct a map T from the Kantorovich plan P (output of ot.emd and then max $x_i$ to $y = argmax_j(P_{ij})$ and then do the interpolation with an optimal map\n",
        "- You have free will, you can change distributions\n"
      ],
      "metadata": {
        "id": "Y38zc52AmMqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Free space\"\"\""
      ],
      "metadata": {
        "id": "Qc71739Eh7w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LOnYoqjTpN4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8orWV9gPpN7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Wasserstein distance as a loss for a Generator:\n",
        "\n",
        "Often, `x_fake = G(z)` are generated from some lower dimensional data $z\\sim Law(z)$, e.g., some sample from a Gaussian distribution $z\\sim N(0, I)$\n",
        "\n",
        "\n",
        "\n",
        "### Core idea for today:\n",
        "\n",
        "In comparison to a GAN, where the generator is training to fool a discriminator network, we train it to minimize the regularized OT distance between\n",
        "- Generated samples $G_\\theta(z)$ from noise $z \\sim \\mathcal{N}(0, I)$\n",
        "- Real data samples $x \\sim \\mu$\n",
        "\n",
        "The OT cost serves as the loss function:\n",
        "$$Loss = OT_\\varepsilon(\\mu, Law(G_\\theta(z)))$$\n",
        "\n",
        "In practice, computing the OT distance between entire distributions (say, full datasets or batches) is expensive, thus for training, at each step we may sample some batches of them and create empirical distributions from batches.\n",
        "\n",
        "### Workflow:\n",
        "1. Sample mini-batch from real data: $x_1, ..., x_n$\n",
        "2. Sample mini-batch from generator: $G(z_1), ..., G(z_n)$\n",
        "3. Compute cost matrix $C$ between generated and real samples\n",
        "4. Compute Sinkhorn distance\n",
        "5. Backpropagate and update generator weights\n",
        "\n",
        "\n",
        "**Important comment:** In this implementation, we will essentially have OT between measures $Unif(X)$ and $Unif(Y)$ where $X = \\{x_1, ..., x_n\\}$ and $Y=\\{G(z_1), ..., G(z_n)\\}$ with a Cost composed of pairwise distances between samples.\n",
        "$$C[i, j] = d(x_{real}[i], x_{fake}[j])^2$$\n",
        "The idea of this is to try to sort of \"match\" the domain $Y$ to $X$.\n",
        "\n",
        "*However, for examples with images, we will not be computing pairwise Wasserstein distance between \"images\" themself (i.e., normalized to be probability distributions) like we did in the barycenter example as it is a bit more expensive. Instead, we will use a cost which is $L_2$ comparison of grayscale images.*\n",
        "$$C[i, j] = ||x_{real}[i] - x_{fake}[j]||^2$$\n"
      ],
      "metadata": {
        "id": "4ueYBTpoe1Cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rewriting Sinkhorn in a tensorflow language\n",
        "@tf.function\n",
        "def sinkhorn_tf(mu, nu, C, eps=0.01, max_iter=100, tol=1e-6):\n",
        "    u = tf.zeros_like(mu)\n",
        "    v = tf.zeros_like(nu)\n",
        "    log_mu = tf.math.log(mu + 1e-8)\n",
        "    log_nu = tf.math.log(nu + 1e-8)\n",
        "\n",
        "    def cond(it, u, v, converged):\n",
        "        return tf.logical_and(it < max_iter, tf.logical_not(converged))\n",
        "\n",
        "    def body(it, u, v, converged):\n",
        "        log_sum_u = tf.math.reduce_logsumexp((v[None, :] - C) / eps, axis=1)\n",
        "        u = eps * log_mu - eps * log_sum_u\n",
        "\n",
        "        log_sum = tf.math.reduce_logsumexp((u[:, None] - C) / eps, axis=0)\n",
        "        v = eps * log_nu - eps * log_sum\n",
        "\n",
        "        P = tf.exp((u[:, None]+ v[None, :] - C) / eps)\n",
        "        # Convergence check\n",
        "        converged =  tf.reduce_sum(tf.abs(tf.reduce_sum(P, axis=1) - mu)) < tol\n",
        "        return it+1, u, v, converged\n",
        "\n",
        "    it0 = tf.constant(0)\n",
        "    converged0 = tf.constant(False)\n",
        "\n",
        "    it, u_final, v_final, _ = tf.while_loop(cond, body, [it0, u, v, converged0])\n",
        "\n",
        "    P = tf.exp((u_final[:, None] + v_final[None, :] - C) / eps)\n",
        "    return P, u_final, v_final\n",
        "\n",
        "@tf.function\n",
        "def sinkhorn_loss(x, y, eps=0.05):\n",
        "    # uniform measures on the sampled batches\n",
        "    n = tf.shape(x)[0]\n",
        "    m = tf.shape(y)[0]\n",
        "    mu = tf.fill([n], 1.0 / tf.cast(n, tf.float32))\n",
        "    nu = tf.fill([m], 1.0 / tf.cast(m, tf.float32))\n",
        "\n",
        "    C = tf.reduce_sum((tf.expand_dims(x, 1) - tf.expand_dims(y, 0)) ** 2, axis=2)\n",
        "    P, U, V = sinkhorn_tf(mu, nu, C, eps=eps)\n",
        "\n",
        "    loss = - eps*tf.reduce_sum(P)\n",
        "    loss += tf.reduce_sum(U*mu)\n",
        "    loss += tf.reduce_sum(V*nu)\n",
        "\n",
        "    # equivalently, could have used\n",
        "    # loss = tf.reduce_sum(P * C) + eps * tf.reduce_sum(P * tf.math.log(P))\n",
        "    # or even\n",
        "    # loss = tf.reduce_sum(P * C)\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "uvTLa-g578ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's see an example where `x_real` are some points clouds in $\\mathbb R^2$"
      ],
      "metadata": {
        "id": "TmshirSOvpee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "latent_dim = 2\n",
        "lr = 1e-3\n",
        "epsilon = 0.1\n",
        "\n",
        "# Real samples (2 blobs)\n",
        "def sample_blobs(batch_size):\n",
        "    centers = np.array([[2, 2], [-2, -2]])\n",
        "    ids = np.random.randint(0, 2, size=batch_size)\n",
        "    noise = 0.3 * np.random.randn(batch_size, 2)\n",
        "    return centers[ids] + noise\n",
        "\n",
        "\n",
        "def build_blobs_generator():\n",
        "    model = tf.keras.Sequential([\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(2)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Training loop\n",
        "blobs_generator = build_blobs_generator()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "losses = []\n"
      ],
      "metadata": {
        "id": "5pMbuPaO_zcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1000\n",
        "batch_size = 128\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    x_real = sample_blobs(batch_size).astype(np.float32)\n",
        "    z = np.random.randn(batch_size, latent_dim).astype(np.float32)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        x_fake = blobs_generator(z)\n",
        "        loss = sinkhorn_loss(tf.convert_to_tensor(x_real), x_fake, epsilon)\n",
        "\n",
        "    grads = tape.gradient(loss, blobs_generator.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, blobs_generator.trainable_variables))\n",
        "    losses.append(loss)\n",
        "\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print(f\"Epoch {epoch+1}: Sinkhorn Loss = {loss:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "vWGGm7OdOPEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"run this cell many times to see many examples\"\"\"\n",
        "\n",
        "x_real = sample_blobs(batch_size)\n",
        "z = np.random.randn(batch_size // 2, latent_dim).astype(np.float32)\n",
        "x_fake = blobs_generator(z).numpy()\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(x_real[:, 0], x_real[:, 1], color='blue', alpha=0.5, label='Real')\n",
        "plt.scatter(x_fake[:, 0], x_fake[:, 1], color='red', alpha=0.5, label='Generated')\n",
        "plt.legend()\n",
        "plt.title(\"Real vs fake with Sinkhorn Loss\")\n",
        "plt.axis('equal')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y-5iUfHcFxcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercises:\n",
        "- Experiment with network architecture (deepness, activations, etc.) and also latent dimensions of `z`\n",
        "- Try to change values of $\\varepsilon$, and also try to change the loss into the 2 other options as indicated in a comment inside `sinkhorn_loss`. Do you see any difference? Why? (Talk to me)\n",
        "- Try to fake some other distributions and learn them by analogy (for instance take a higher dimension for `x_real`). How big the latent dimension of `z` must be to produce some meaningful results?"
      ],
      "metadata": {
        "id": "iyrO-CaA9mhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Free space for experiments\"\"\""
      ],
      "metadata": {
        "id": "8B9ykizT_G57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8g7VFdj1_HIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oniCVtwS_K1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LnpWYaN7_K3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now let's try to generate some images of digits!"
      ],
      "metadata": {
        "id": "PZNRlcWOJrVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "def build_mnist_generator():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dense(28*28, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "(mnist_digits, mnist_labels), (_, _) = mnist.load_data()"
      ],
      "metadata": {
        "id": "S2d455yhJvvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 64\n",
        "batch_size = 32\n",
        "epsilon = 0.01\n",
        "\n",
        "digit = 3\n",
        "\n",
        "number_generator = build_mnist_generator()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "\n",
        "# Load MNIST and filter only digit, e.g. \"3\"\n",
        "x_train = mnist_digits[mnist_labels == digit]\n",
        "x_train = x_train.astype(np.float32) / 255.0\n",
        "x_train = x_train.reshape((-1, 28*28))  # Flatten images\n",
        "\n",
        "# Select subset for training\n",
        "x_train = x_train[:1000]\n",
        "\n",
        "losses = []"
      ],
      "metadata": {
        "id": "f8xkrPHEKPaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "n_epochs = 800\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    idx = np.random.choice(len(x_train), batch_size, replace=False)\n",
        "    x_real = tf.convert_to_tensor(x_train[idx])\n",
        "    z = tf.random.normal((batch_size, latent_dim))\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        x_fake = number_generator(z)\n",
        "        loss = sinkhorn_loss(tf.convert_to_tensor(x_real), x_fake, epsilon)\n",
        "\n",
        "    grads = tape.gradient(loss, number_generator.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, number_generator.trainable_variables))\n",
        "    losses.append(loss)\n",
        "\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print(f\"Epoch {epoch+1}: Sinkhorn Loss = {loss:.4f}\")"
      ],
      "metadata": {
        "id": "zWb7wU977ROH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"run this cell many times to see many examples\"\"\"\n",
        "\n",
        "z = tf.random.normal((1, latent_dim))\n",
        "generated_image = number_generator(z).numpy().reshape(28, 28)\n",
        "\n",
        "plt.imshow(generated_image, cmap='gray')\n",
        "plt.title(f\"Generated digit '{digit}'\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0Dl70IQqKbuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercises\n",
        "- Obviously, try different digits, experiment with network design, latent dimension of `z`, batch_size, values of $\\varepsilon$, etc.\n",
        "- Try some maybe different datasets other than `mnist`, for example `fashion-mnist`, `cifar-10`\n",
        "- What happens if we don't restrict to a single label in a dataset, and just try to generate any sample from a chosen dataset, e.g. `mnist`. Does it generate an actual and meaningful \"digit\"?"
      ],
      "metadata": {
        "id": "-uOZk4wM_8Mb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWoNBog7Fs1Q"
      },
      "outputs": [],
      "source": [
        "\"\"\"Free space for experiments\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yYhSVHqDBo4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g183e-h3Bo6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4wFUcM4bBo85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampling with Langevin Dynamics\n",
        "\n",
        "Here, we want to first assume that out goal is to sample from a complex target distribution $\\pi$ with density\n",
        "$p(x) \\propto e^{-V(x)} $ where $V(x)$ is some potential\n",
        "(e.g., $\\pi$ is a Bayesian posterior, Gibbs distribution). However, we might not be able to sample directly.\n",
        "\n",
        "### Strategy\n",
        "We simulate the evolution of a probability distribution $\\rho_t$ with densitied $p_t(x)$\n",
        "under a **Wasserstein gradient flow** that minimizes the KL divergence to the target distribution\n",
        "$$\\frac{\\partial p_t}{\\partial t} = \\nabla\\cdot (p_t \\nabla \\log (\\frac{p_t}{p}))$$\n",
        "This is equivalent to the Fokker-Planck equation, and it corresponds to the Langevin dynamics SDE:\n",
        "$$d X_t = -\\nabla (V(X_t)) dt +\\sqrt{2} d B_t$$\n",
        "where $(B_t)_{t>0}$ is a standard Brownian motion.\n",
        "\n",
        "**Theorem:** It is known that is $\\nabla V$ is Lipshitz continuous, then there exists a unique solution for any initial conditions, and its stationary distribution is $\\pi$.\n",
        "\n",
        "\n",
        "### Idea:\n",
        "So, by discretizing this SDE, we can evolve a set of particles toward the target distribution $\\pi$.\n",
        "\n",
        "$$X_{t+1} = X_t - \\eta \\nabla (V(X_t)) + \\sqrt{2\\eta} N(0, I) $$\n",
        "where $\\eta$ is a stepsize.\n",
        "\n"
      ],
      "metadata": {
        "id": "HocVbQ-eH4Ys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "For the first example, letâ€™s define a potential\n",
        "$V(x) = \\frac12 ||x - a||^2$, then $\\pi$ will be a Gaussian with mean at $a$ and variance 1."
      ],
      "metadata": {
        "id": "M4II-RujK6gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Target potential: V(x) = 0.5 * ||x - a||^2\n",
        "def V(x, a):\n",
        "    return 0.5 * tf.reduce_sum((x - a) ** 2, axis=1)\n",
        "\n",
        "# Gradient of V\n",
        "def grad_V(x, a):\n",
        "    return x - a  # V'(x) = x - a\n",
        "\n",
        "# Langevin dynamics sampler (ULA)\n",
        "def langevin_sampler(a, mu_0, steps=1000, step_size=0.01, n_particles=100):\n",
        "    d = a.shape[0]\n",
        "    x = mu_0(n_particles, d)\n",
        "\n",
        "    samples = []\n",
        "\n",
        "    for _ in range(steps):\n",
        "        noise = tf.random.normal(shape=tf.shape(x))\n",
        "        grad = grad_V(x, a)\n",
        "        x = x - step_size * grad + tf.sqrt(2.0 * step_size) * noise\n",
        "        samples.append(x.numpy())\n",
        "\n",
        "    return tf.stack(samples)  # shape: (steps, n_particles, d)"
      ],
      "metadata": {
        "id": "SUGddgWCNLgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = tf.constant([7.0, -4.0])\n",
        "# Initial particles are uniform over (-10, 10)^2\n",
        "mu_0 = lambda n_particles, d: tf.random.uniform((n_particles, d), minval=-10, maxval=10)\n",
        "\n",
        "samples = langevin_sampler(a, mu_0, steps=500, step_size=0.01, n_particles=100)"
      ],
      "metadata": {
        "id": "xjZVbIRuQZbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now let's see how it evolves\n",
        "\n",
        "def plot_samples_at_step(step=0):\n",
        "    data = samples[step]\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.scatter(data[:, 0], data[:, 1], alpha=0.5)\n",
        "    plt.xlim(-10, 10)\n",
        "    plt.ylim(-10, 10)\n",
        "    plt.title(f\"Step {step}\")\n",
        "    plt.xlabel(\"x1\")\n",
        "    plt.ylabel(\"x2\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "interact(plot_samples_at_step, step=IntSlider(min=0, max=len(samples)-1, step=10, value=0));"
      ],
      "metadata": {
        "id": "E-DP1FKxMPv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercises\n",
        "- Now, try to sample initial particles $X_0$ from other distributions. Do we still converge to a specified Gaussian $N(a, I)$?\n",
        "- Also, you can try choosing different target distributions, as long as its density is of form $p(x)\\propto e^{-V(x)}$."
      ],
      "metadata": {
        "id": "3VbRfOaUZIC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Free space for experiments\"\"\""
      ],
      "metadata": {
        "id": "tE6EopRYZG4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7D47Q0yEaC1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aabboCqFaC3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now, what to do if you want other distributions?\n",
        "Suppose we want to flow towards some other distributions, e.g. distributions of pictures of digits. What can we try doing?\n",
        "\n",
        "We can try to assume that their distribution $d\\pi = p(x)dx$ with has density $p(x)\\propto e^{-V(x)}$. How do we choose $V(x)$?\n",
        "\n",
        "#### Option 1: Use a Trained Classifier for ( $\\nabla \\log p(X_t)$)\n",
        "Train a neural network to try to \"learn\" the distribution of data at time $t$ where we gradually add noice to the distribution:\n",
        "$$s_\\theta(x,t) \\approx -\\nabla\\log p_t(x)$$\n",
        "Then treat the log-probability output as the potential $V$ and then use it for the Langevin system.\n",
        "$$X_{t+1} = X_t - \\eta  (s_\\theta(x,t)) + \\sqrt{2\\eta} N(0, I)$$\n",
        "\n",
        "#### Option 2: Classical Langevin Dynamics / Wasserstein Gradient Flow\n",
        "\n",
        "Choose an energy potential $V$ such that for real data we have low energy, and otherwise higher. Then $\\nabla V$ pulls you toward high-probability regions (like digits), and noise helps you explore.\n",
        "\n",
        "Key idea:\n",
        "- You manually define an energy landscape $V$ (e.g., low energy near digit images, high elsewhere).\n",
        "- Sampling = run Langevin dynamics to reach low-energy areas (realistic samples).\n",
        "\n",
        "Particular algorithm: Langevin Monte Carlo\n",
        "\n",
        "(Good implementation is slightly time-consuming, thus if you wish, you could try by yourself)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bUBVekrLYU89"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tMaNSdOhdZNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "703tM7HtdZQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A1zR-Svndaee"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}